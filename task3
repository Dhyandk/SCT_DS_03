import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('/content/bank-full.csv', sep=';')
df.head()

# Encode categorical columns
categorical_cols = df.select_dtypes(include=['object']).columns
le = LabelEncoder()
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

# Define features and target
X = df.drop('y', axis=1)
y = df['y']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = DecisionTreeClassifier(criterion='entropy', max_depth=5, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Accuracy Score:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

importances = model.feature_importances_
features = X.columns
indices = np.argsort(importances)[::-1]

plt.figure(figsize=(10, 6))
sns.barplot(x=importances[indices], y=features[indices])
plt.title("Feature Importance in Decision Tree")
plt.show()

!pip install graphviz pydotplus

from sklearn.tree import export_graphviz
import graphviz

dot_data = export_graphviz(model,
                           out_file=None,
                           feature_names=X.columns,
                           class_names=['No', 'Yes'],
                           filled=True,
                           rounded=True,
                           special_characters=True,
                           max_depth=3)  # limit visible depth

graph = graphviz.Source(dot_data)
graph.render("decision_tree", format='png', cleanup=False)  # Saves image as 'decision_tree.png'
graph

"""üîç Insights**
---
The Decision Tree Classifier was used to predict whether a customer will subscribe to a term deposit based on demographic and behavioral features from the Bank Marketing dataset.

Top features influencing the decision (based on feature importance analysis):
--
duration: Duration of the last contact call.
poutcome: Outcome of the previous marketing campaign.
previous: Number of contacts performed before this campaign.
campaign: Number of contacts during the current campaign.
balance: Customer's average yearly account balance.
Limiting the visualization to the top 3 levels of the tree made it easier to interpret and revealed key decision rules based on duration and poutcome.

The Model achieved an accuracy score of 89%
--
Model Interpretability is a major advantage of Decision Trees, making them suitable for understanding customer behavior.

To improve the model and reduce overfitting:
--
Limit tree depth (max_depth)
Use Cost-Complexity Pruning (ccp_alpha)
Try ensemble methods like Random Forests or Gradient Boosted Trees
This model can serve as a baseline for more advanced customer targeting strategies in marketing campaigns.
"""
